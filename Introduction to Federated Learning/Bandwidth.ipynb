{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0864134",
   "metadata": {},
   "source": [
    "# Bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b96f7b",
   "metadata": {},
   "source": [
    "#### 1. Load imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc861ea-21bb-45a8-8196-a5b0d0ace374",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from flwr.client.mod import parameters_size_mod\n",
    "\n",
    "from utils5 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a45205f",
   "metadata": {},
   "source": [
    "#### 2.  Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d9e47",
   "metadata": {},
   "source": [
    "*  Initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb82dbc0-cb85-408c-ac0f-265af010bfea",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"EleutherAI/pythia-14m\",\n",
    "    cache_dir=\"./pythia-14m/cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b51a54",
   "metadata": {},
   "source": [
    "Find more information about [EleutherAI/pythia-14m](https://huggingface.co/EleutherAI/pythia-14m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6159b23d",
   "metadata": {},
   "source": [
    "* Get some Model values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973616a4-8b73-41b8-bc9d-4abbbc74502d",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m: Model size is: 53 MB\n"
     ]
    }
   ],
   "source": [
    "vals = model.state_dict().values()\n",
    "total_size_bytes = sum(p.element_size() * p.numel() for p in vals)\n",
    "total_size_mb = int(total_size_bytes / (1024**2))\n",
    "\n",
    "log(INFO, \"Model size is: {} MB\".format(total_size_mb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccf308f",
   "metadata": {},
   "source": [
    "* Define the FlowerClient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea907386-ef03-46a5-bbe3-05414dee7b4b",
   "metadata": {
    "height": 404
   },
   "outputs": [],
   "source": [
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(self, net):\n",
    "        self.net = net\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        set_weights(self.net, parameters)\n",
    "        # No actual training here\n",
    "        return get_weights(self.net), 1, {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        set_weights(self.net, parameters)\n",
    "        # No actual evaluation here\n",
    "        return float(0), int(1), {\"accuracy\": 0}\n",
    "\n",
    "\n",
    "def client_fn(context: Context) -> FlowerClient:\n",
    "    return FlowerClient(model).to_client()\n",
    "\n",
    "\n",
    "client = ClientApp(\n",
    "    client_fn,\n",
    "    mods=[parameters_size_mod],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffa0f64",
   "metadata": {},
   "source": [
    "* Define the custom strategy: BandwidthTrackingFedAvg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95213409-e47d-4415-b69f-9bfb5498aaf4",
   "metadata": {
    "height": 557
   },
   "outputs": [],
   "source": [
    "bandwidth_sizes = []\n",
    "\n",
    "\n",
    "class BandwidthTrackingFedAvg(FedAvg):\n",
    "    def aggregate_fit(self, server_round, results, failures):\n",
    "        if not results:\n",
    "            return None, {}\n",
    "\n",
    "        # Track sizes of models received\n",
    "        for _, res in results:\n",
    "            ndas = parameters_to_ndarrays(res.parameters)\n",
    "            size = int(sum(n.nbytes for n in ndas) / (1024**2))\n",
    "            log(INFO, f\"Server receiving model size: {size} MB\")\n",
    "            bandwidth_sizes.append(size)\n",
    "\n",
    "        # Call FedAvg for actual aggregation\n",
    "        return super().aggregate_fit(server_round, results, failures)\n",
    "\n",
    "    def configure_fit(self, server_round, parameters, client_manager):\n",
    "        # Call FedAvg for actual configuration\n",
    "        instructions = super().configure_fit(\n",
    "            server_round, parameters, client_manager\n",
    "        )\n",
    "\n",
    "        # Track sizes of models to be sent\n",
    "        for _, ins in instructions:\n",
    "            ndas = parameters_to_ndarrays(ins.parameters)\n",
    "            size = int(sum(n.nbytes for n in ndas) / (1024**2))\n",
    "            log(INFO, f\"Server sending model size: {size} MB\")\n",
    "            bandwidth_sizes.append(size)\n",
    "\n",
    "        return instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0166979-2cc2-44ac-89dd-e9bc05c941e9",
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "params = ndarrays_to_parameters(get_weights(model))\n",
    "\n",
    "def server_fn(context: Context):\n",
    "    strategy = BandwidthTrackingFedAvg(\n",
    "        fraction_evaluate=0.0,\n",
    "        initial_parameters=params,\n",
    "    )\n",
    "    config = ServerConfig(num_rounds=1)\n",
    "    return ServerAppComponents(\n",
    "        strategy=strategy,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "\n",
    "server = ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4e469b",
   "metadata": {},
   "source": [
    "* Run the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4cddbee-f017-4ad1-b6e1-2a81b857fcef",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m: Starting Flower ServerApp, config: num_rounds=1, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m: \n",
      "\u001b[92mINFO \u001b[0m: [INIT]\n",
      "\u001b[92mINFO \u001b[0m: Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m: Evaluating initial global parameters\n",
      "\u001b[92mINFO \u001b[0m: \n",
      "\u001b[92mINFO \u001b[0m: [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m: Server sending model size: 53 MB\n",
      "\u001b[92mINFO \u001b[0m: Server sending model size: 53 MB\n",
      "\u001b[92mINFO \u001b[0m: configure_fit: strategy sampled 2 clients (out of 2)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m I0000 00:00:1723270613.254346     156 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n",
      "\u001b[2m\u001b[36m(ClientAppActor pid=378)\u001b[0m \u001b[92mINFO \u001b[0m: {'fitins.parameters': {'parameters': 14067712, 'bytes': 56280718}}\n",
      "\u001b[2m\u001b[36m(ClientAppActor pid=378)\u001b[0m \u001b[92mINFO \u001b[0m: Total parameters transmitted: 56280718 bytes\n",
      "\u001b[92mINFO \u001b[0m: aggregate_fit: received 2 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m: Server receiving model size: 53 MB\n",
      "\u001b[92mINFO \u001b[0m: Server receiving model size: 53 MB\n",
      "\u001b[92mINFO \u001b[0m: configure_evaluate: no clients selected, skipping evaluation\n",
      "2024-08-10 06:16:58,632\tWARNING worker.py:2037 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffd4412040d3e1e41b5d5f6fd501000000 Worker ID: 7bae478018dfa31575af003d9347e6f6b5010d08b51abdc7349c88f3 Node ID: ed0a93bc651ac129de8819fc7460aff87153f58f3baf314c62687147 Worker IP address: 172.30.129.9 Worker port: 41517 Worker PID: 377 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[92mINFO \u001b[0m: \n",
      "\u001b[92mINFO \u001b[0m: [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m: Run finished 1 round(s) in 8.33s\n",
      "\u001b[92mINFO \u001b[0m: \n",
      "\u001b[2m\u001b[36m(ClientAppActor pid=377)\u001b[0m \u001b[92mINFO \u001b[0m: {'fitins.parameters': {'parameters': 14067712, 'bytes': 56280718}}\n",
      "\u001b[2m\u001b[36m(ClientAppActor pid=377)\u001b[0m \u001b[92mINFO \u001b[0m: Total parameters transmitted: 56280718 bytes\n"
     ]
    }
   ],
   "source": [
    "run_simulation(server_app=server,\n",
    "               client_app=client,\n",
    "               num_supernodes=2,\n",
    "               backend_config=backend_setup\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d64b93",
   "metadata": {},
   "source": [
    "* Log how much bandwidth was used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d49b2f6d-91eb-4161-b86e-74d897cb725a",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m: Total bandwidth used: 212 MB\n"
     ]
    }
   ],
   "source": [
    "log(INFO, \"Total bandwidth used: {} MB\".format(sum(bandwidth_sizes)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
